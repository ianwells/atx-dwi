---
title: "Messing Around with DWI Arrests, TABC Mixed Beverage Taxes, and Ridesharing, in Austin, TX"
output: html_document
date: 2016-01-12
---

#DRAFT DRAFT DRAFT

(Playing along at home?  Here's what you'll need:)

```{r}
(.packages())
select <- dplyr::select
```

```{r include = FALSE}    
library(ggplot2)
library(dplyr)
library(knitr)
library(lubridate)
library(reshape)
library(CausalImpact)
library(rddtools)

select <- dplyr::select 
```

##Background

So, as you may or may not know, Uber and Lyft, America's two premiere ridesharing services, stopped offering rides in Austin, TX as of May 9, 2016 after a contentious public vote which you can read about here: http://www.bizjournals.com/austin/news/2016/05/07/uber-lyft-defeated-in-prop-1-referendum.html

A common argument is that the presence of ride-sharing services such as Uber or Lyft decreases the incidence of drunken driving in a city in which they operate; it sounds reasonable enough to say that if there are more options for getting home after drinking, at least one of them might be more attractive than driving home in a potentially compromised state.

There have been a few articles and a couple papers studying the effect of the availability of ridesharing services on drunk driving (see references below), but in my opinion they fall short in one or more ways:

*They fail to count all available records of drunk driving (like if, say, a study only counted drunk driving fatalities)
*They fail to account for trends in drunk driving frequency (like if, say, more people than average drove drunk on certain holidays), or aggregate time-series data too broadly
*They fail to control for exogenous effects on drunk driving frequency (like if, say, the population of a core drinking demographic increased)
*They fail to properly treat the regression disontinuity between data before and after ridesharing was available in Austin (more on this later)

I've managed to get ahold of three datasets that I think will be helpful:
  
1. APD Incident Reports for 2008-2011, Jan-July 2015, Jan-July 2016 (including a number of intoxication-related crimes)
2. TABC Mixed Beverage Tax Receipts for Austin, 2008-2016.
3. APD Chief's Monthly Reports, 2010-2016, with numbers of DWI and Narcotics arrests.

The APD (Austin Police Department) Incident Reports are publicy available for years 2008-2011, and for 'YTD' which supposedly includes the last 18 months of reported crime in Austin but actually only includes 8 months of 2016 and a small number of reports from 2015.  I was able to obtain an archived version of the data from late 2015, which has more complete data for Jan-Jul 2015.  

The APD Chief's Monthly Report is a high-level summary of crime in Austin for the month, including counts for high-profile crimes including DWI.  It has no time-of-day or location information, and no info other than just that a DWI occured, but is another source of data to compare to the incident reports, which may be spotty.  It additionally contains data on narcotics arrests, which we can check to see if we can use it as a control variable - narcotics crimes may follow the same broad trends as DWI, but are potentially unaffected by ridesharing.

It's worth noting that the study of DWI arrest numbers as a proxy for the indicence of drunk driving is an open area of research - some studies prefer drunk driving crashes, although Austin has not released enough monthly statistics on drunk driving crashes to make it a usable variable in these models.  We'll assume DWI arrests are a viable proxy for drunk driving here.

The TABC (Texas Alcoholic Beverage Commission) Mixed Beverage Tax is a fixed tax charged on alcoholic beverages sold by a holder of a Mixed Beverage License in the state of Texas.  I suspect that Mixed Beverage Tax revenues are correlated with Units of Alcohol Consumed in Austin, and we'll check to see if any relationship exists between these tax revenues and DWI counts.

Here are some important dates:

*March 2013 : Uber is around for SXSW.
*March 2014 : Uber is around for SXSW.

*June 3, 2014: Lyft launches in Austin.
https://twitter.com/AustinLyft/status/473857057720766464

*June 4, 2014 : Uber launches in Austin.
https://twitter.com/Uber_ATX/status/474199740217716736

*May 9, 2016: Uber and Lyft stop giving rides in Austin.

You can find the same data I used here:
* https://data.austintexas.gov/browse?q=apd&sortBy=relevance
* https://www.comptroller.texas.gov/transparency/open-data/search-datasets/
* https://austintexas.gov/page/chiefs-monthly-reports
* https://durs.carto.com/tables/apd_incident_extract_2015csv/public

Or you can pull it from my Github, 
*http://www.github.com/ianwells

Alright, Let's get to it.

#The Data

##DWI Arrests

```{r include = FALSE}
setwd('/Users/ianwells/sandbox/atx-dwi/')
dwi.raw <- read.csv('data/dwi-raw-nohitandrun-nodre.csv')

sapply(dwi.raw,class)
head(dwi.raw)
dwi.raw$date <- parse_date_time(dwi.raw$date,'%Y-%m-%d %H:%M:%S',tz ='America/Chicago')

#har.raw <- read.csv('data/hitandrun-raw.csv')

#sapply(har.raw,class)
#head(har.raw)
#har.raw$date <- parse_date_time(har.raw$date,'%Y-%m-%d %H:%M:%S',tz ='America/Chicago')

tabc.raw <- read.csv('data/tabc-raw.csv')
sapply(tabc.raw,class)
tabc.raw$date <- parse_date_time(tabc.raw$date,'%Y-%m-%d %H:%M:%S', tz = 'America/Chicago')

apd.raw <- read.csv('data/chief-report-raw.csv')
narc.raw <- read.csv('data/narc-raw.csv')
apd.raw <- merge(apd.raw,narc.raw, by = 'date')

```

Here's a quick glimpse of what the raw APD data looks like:

```{r}
sample_n(dwi.raw,10)
```

Cool.  We have a bunch of crimes, pre-filtered to intoxication-related ones and the time they approximately occured.

You can check this against the full list of crimes availible to commit in Austin, which is available on that same Austin Data Portal site, I'm not including it here because it's quite long, please trust that I've got all of them.

```{r}
levels(dwi.raw$crime)
```

If you're familar with this data set you'll notice I've removed a few crimes:

1) `BOATING WHILE INTOXICATED` - This rare crime doesn't seem like it could be avoided by hailing an Uber.
2) `DWI - DRUG RECOGNITION EXPERT` - You don't have to have any alcohol in your system to be charged with a DWI: in this case, an officer certified in recognizing the effects of some other substance has decided you shouldn't be driving.  While I imagine you could avoid trouble by hailing a ride in this case, I don't think I could relate it to bar sales, so this stat is going to be relegated to some other study.
3) `CRASH/LEAVING THE SCENE` - This is a tricky one.  Certainly a non-zero percentage of hit-and-run offenses are commited by drunk drivers, but this charge doesn't offer any more details, and worse, there are more of this kind of crime than all the others put together.  How should we count them, if at all?  For now, let's leave them all out, as they're not counted in offical DWI tallies.  We might be able to project how many of these crimes involve alcohol at a later date, based on when and where they occured, but we'll leave that for another analysis.

Let's take a look at the relative frequency of these offenses.  Periodicity can be exploited later on.

```{r warning= FALSE}
qplot(hour(date), data=dwi.raw, geom="histogram", bins = 24)
```

Perhaps not surprisingly, nearly all DWI offenses occur between 8 PM and 5 AM.

```{r warning= FALSE}
qplot(wday(date,label = TRUE), data=dwi.raw)
```

Again, looks like Fridays and Saturdays are popular days to get caught drunk driving.

We're going to look at the whole series, and then a histogram of monthly data to see if there's a seasonal trend (you might expect more DWI's to occur in December,January, or July months with holiday weekends that involve a lot of drinking)

Note: we have incomplete data for July 2015 and August 2016, as well as an old offense in 2014, so we'll remove those months now.  We'll leave 2015 and 2016 out of the monthly histogram because we only have data from a few months for those years.

```{r  warning= FALSE}
dwi.raw$year <- year(dwi.raw$date)
dwi.raw$month <- month(dwi.raw$date)
  
dwi.monthly <- dwi.raw %>% group_by(year,month)
dwi.monthly <- summarize(dwi.monthly,count=n())
dwi.monthly$date <- as.Date(paste0(dwi.monthly$year,'-',dwi.monthly$month,'-01'))

dwi.monthly <- dwi.monthly[-56,] 
dwi.monthly <- dwi.monthly[-63,] 
dwi.monthly <- dwi.monthly[-49,] 

label.months <- c('J','F','M','A','M','J','J','A','S','O','N','D')

ggplot(data=dwi.monthly, aes(x = date, y = count, group = year(date), color = factor(year(date)))) + geom_point()

#ggplot(data=dwi.monthly, aes(x = (month(date)), y = count, group = year(date), color = factor(year(date)))) + geom_smooth(se = FALSE) + scale_x_continuous( breaks = 1:12, labels = label.months)

qplot(factor(month(date)), data=filter(dwi.monthly,date < '2015-01-01'), geom="bar", weight = count)
```

So we can get a sense that, for the years 2008-2011, monthly DWI's seem to be decreasing (good job APD), and as for a monthly trend we see an annual peak in December-January, as well as something going on in August-October.

Now, I'm curious to see how the Chief's Monthly Report compares.

```{r}

apd.raw$date <- as.Date(apd.raw$date)
apd.raw$year <- year(apd.raw$date)
apd.raw$month <- month(apd.raw$date)

apd.monthly <- apd.raw
  
ggplot(data=apd.monthly, aes(x = date, y = count, color = factor(year))) + geom_point() 

qplot(factor(month(date)), data=apd.monthly, geom="bar", weight = count)
```

This has a similar December-January peak.  Let's plot both at the same time.


```{r}
both.monthly <- merge(apd.monthly,dwi.monthly, by = 'date', all = TRUE);
both.monthly$apd <- both.monthly$count.x
both.monthly$dwi <- both.monthly$count.y

both.monthly <- both.monthly %>% select(date,apd,dwi)
both.melted <-melt(both.monthly, id.vars="date")

ggplot(data=both.melted, mapping = aes(x=date,y=value, color = variable)) + geom_point()

```

Okay, so it looks like the chief's report is a little different than the incident report database.  That's annoying, but based on the disclaimers on both websites, and that the chief's report is updated more often, I'm going to call it and say the chief's data is what I'm using moving forward.  I'd like those extra two years of data that the database has though - and since the data is so historic, and the difference is so consistent, what I'm going to do is nudge the data up slightly so that it lines up, and use that.

```{r}

both.monthly$diff <- both.monthly$apd - both.monthly$dwi
mean(filter(both.monthly,date < '2012-01-01', date > '2010-01-01')$diff)

both.monthly.nudge <- both.monthly
both.monthly.nudge$dwi = both.monthly.nudge$dwi + 15

both.melted.nudge <-melt(both.monthly.nudge, id.vars="date")
ggplot(data=both.melted.nudge, mapping = aes(x=date,y=value, color = variable)) + geom_point()

```

Alright, that's a little cleaner.  

Let's take a second to look at the narcotics arrest data too.

```{r}

ggplot(data=apd.monthly, aes(x = date, y = narc, color = factor(year))) + geom_point() 

qplot(factor(month(date)), data=apd.monthly, geom="bar", weight = narc)

```

Not a lot to say here - there's a strong January but not a lot else that lines up with the DWI data.  We'll see if they're correlated later.

I've saved out our final data set here to save time.

```{r}
dwi <- read.csv('data/dwi-final.csv')
sapply(dwi,class)
dwi$date <- as.Date(dwi$date)
dwi$count <- as.numeric(as.character(dwi$count))
dwi$narc <- as.numeric(as.character(dwi$narc))
dwi$treatment <- as.factor(dwi$treatment)
```

## TABC Mixed Beverage Tax

Let's have a look at what's going on with the TABC data.  Now, I've already done a bit of work with this data set in the past (see tabcmap.s3-website-us-east-1.amazonaws.com), so I've just pulled values from my database of monthly revenues calculated from monthly tax receipts.

Here's what a few rows look like:

##ADJUST FOR INFLATION

```{r}
sample_n(tabc.raw,10)
```

Let's do the same all-time and then year-over-year plots to get a sense of what's in there.

There's a lone data point from 1999, let's clean that up too, and let's scale revenues into the millions of dollars.

```{r warning=FALSE}
tabc.monthly <- tabc.raw[-40,] 

tabc.monthly$date <- as.Date(tabc.monthly$date)

tabc.monthly$year <- year(tabc.monthly$date)
tabc.monthly$month <- month(tabc.monthly$date)
tabc.monthly$rev <- round(tabc.monthly$rev/1000000,2)

ggplot(data=tabc.monthly, aes(x = date, y = rev, group = year, color = factor(year))) + geom_point()

qplot(factor(month(date)), data=tabc.monthly, geom="bar", weight = rev)
```

Look at that growth!  Notice that spike in March?  That's SXSW, a major annual music, technology, and film conference.  See that spike in October?  That's Austin City Limits, a major music festival.  This data is already good to go, so let's rename it to keep things clean and keep going.

```{r}
tabc <- select(tabc.monthly,date,rev)
```

##DWI per $MM

Let's see about a new variable - DWI per Millions of Dollars Bar Revenue.

```{r warning = FALSE}
dwi <- merge(dwi,tabc, by = 'date')

dwi$count_per_mm <- dwi$count/dwi$rev

ggplot(data=dwi, aes(x = date, y = count_per_mm, group = year(date), color = factor(year(date)))) + geom_point()

ggplot(data=dwi, aes(x = rev, y = count, color = year(date))) + geom_point() + geom_smooth(method = "lm")

summary(lm(data = dwi, count ~ rev))

```

So while DWI/$MM is clearly decreasing, it would appear that there's no correlation between DWI counts and bar revenue.  We'd maybe expect to see annual DWI spikes in March if there was, but we don't.

#The Analysis

Alright, so now that we've got some data, what can we do with it?  How can we establish one way or the other that the advent of ridesharing affected the number of DWI arrests in Austin?  There are a few things I'm going to look at.

##Holt-Winters Forecasting

Holt-Winters Forecasting is a method of predicting time-series data that has strong periodic components.  We saw above that our DWI data has some yearly trends in it - the data shows it, and you can even Google around for articles about DWI in Austin and see for yourself that January is a special month for the police (look for 'no-refusal weekend').

What I'm going to do is feed DWI data from January 2008 - May 2014 (the last month before Uber and Lyft were widely available in Austin) into a Holt-Winters model and forecast DWI counts for the next few months.  I'm then going to compare those to the actual data, with the reasoning that since the model takes into account yearly cycles and broad trends, it will come up with a prediction for DWI counts as if Uber and Lyft never arrived.  We can call the difference the number of DWI's ridesharing prevented, with some degree of confidence.

```{r}


dwi.pre <- filter(dwi,date < '2014-05-31')
dwi.post <- filter(dwi,date > '2014-05-31')

dwi.pre.count.ts <- ts(dwi.pre$count, start = c(2008, 1), frequency = 12)
dwi.post.count.ts <- ts(dwi.post$count, start = c(2014, 6), frequency = 12)
#dwi.rev.ts <- ts(dwi$rev, start = c(2008, 1), frequency = 12)

#plot(dwi.pre.count.ts)

dwi.pre.count.hw <- HoltWinters(dwi.pre.count.ts)
#plot(dwi.pre.count.hw)

#dwi.rev.hw <- HoltWinters(dwi.rev.ts)
#plot(dwi.rev.hw)

forecast <- predict(dwi.pre.count.hw, n.ahead = 8, prediction.interval = T, level = 0.9)
plot(dwi.pre.count.hw, forecast)
lines(dwi.post.count.ts,col='green')

```

The forecast is pretty rough - the data may not have been periodic enough for this model to be effective.  For the first 8 months Uber and Lyft were available in Austin, the real cumulative DWI count was 158 less than the predicted count - about 4%.  For the first 3 months, where our confidence interval is much tighter, we're only 24 DWI's less than predicted - a decrease of 1.5%.

Let's try the other side - fit a model from data starting from Uber's first month, and then stopping on Uber's last month.  If the prediction is lower than the real data, we might be able to say that ridesharing was having an effect, but there was some other factor contaminating the data from 2008-2014 (say an organized campaign against drunk driving or something).

We're going to cheat a tiny bit because we need another sample of data (Holt-Winters requires 2 full years of data here) - so I'm going to say Uber actually opened for business a month earlier than it did.  Hopefully the first month was relatively slow as people started adopting to ridesharing, and it won't make much of a difference to stretch the effect out over one more month in addition the 23 months Uber and Lyft were in Austin.

```{r}

dwi.during <- dwi %>% filter(date >= '2014-5-01') %>% filter(date < '2016-05-01')
dwi.exit <- filter(dwi,date >= '2016-05-01')

dwi.during.count.ts <- ts(dwi.during$count, start = c(2014, 5), frequency = 12)
dwi.exit.count.ts <- ts(dwi.exit$count, start = c(2016, 5), frequency = 12)

dwi.during.count.hw <- HoltWinters(dwi.during.count.ts)

forecast <- predict(dwi.during.count.hw, n.ahead = 6, prediction.interval = T, level = 0.9)
plot(dwi.during.count.hw, forecast)
lines(dwi.exit.count.ts,col='green')

```

This looks better - the model fits better, the confidence interval is tighter, and the treatment effect is 80 DWI's over the first 3 months, a decrease of about 6%.  In other words, not having Uber and Lyft may have resulted in an additional 80 arrests.  Maybe more convincing than the initial effect, but let's see if we can do better.

##Bayesian Structural Time-series
###With Google CausalImpact

Google has a great library for testing the causal effect of a given treatment on a time series.  Designed for testing the effectiveness of advertising, I think this library will help us out - it's designed for inferrence on non-experimental data, which is what we've got.

Typically, establishing causation requires good experimental design.  We don't have the resources to do a city-scale ridesharing/drunk-driving experiment, so we'll have to use some fancy math and our existing data.

For this, we require a response and predictor - let's use DWI count as a response, and TABC revenue as a control, as it appears TABC sales growth is linear through the period Uber was available in Austin.

I'm going to take a relatively well-behaved stretch of data around the treatment - let's look at the time-series again, with the treatment area in grey:

```{r}
uber <- geom_rect(mapping = aes(xmin=as.Date('2014-06-01'), xmax = as.Date('2016-05-01')), ymin=-Inf, ymax=+Inf, color="grey20", alpha=0.3, inherit.aes = FALSE)

ggplot(data=dwi, mapping = aes(x=date,y=count, color = year(date))) + uber + geom_point() + geom_smooth(se = FALSE)

```

It appears that there's an inflection around March 2012 - I don't know what caused it, and it happened before Uber showed up, so whatever it is it's outside the scope of this model.  CausalImpact should be able to take the downward trend in DWI and isolate the effect of the treatment in June 2014, if it exists.

We're also limiting the TABC data, so let's look at that, and the correlation for the time frame:

```{r}

ggplot(data=filter(filter(dwi, date > '2012-04-01'),date < '2016-05-09'), mapping = aes(x=date,y=rev, color = year(date))) + uber + geom_point() + geom_smooth(method = 'lm', se = FALSE)

ggplot(data=filter(filter(dwi, date > '2012-04-01'),date < '2016-05-09'), mapping = aes(x=rev,y=count)) + geom_point() + geom_smooth(method = 'lm')

summary(lm(data=filter(filter(dwi, date > '2012-04-01'),date < '2016-05-09'), count ~ rev))

```

What we care about is that there is a correlation between TABC revenue and DWI arrests, and that there does not appear to be a discontinuity in the TABC revenue around when we expect a discontinuity to appear in the DWI data. The correlation exists, but it is negative, and although it looks like TABC sales are unaffected by ridesharing, we can do a quick check with our Holt-Winters method.

(If you're running this at home note that you'll need the development version of `devtools` to install `CausalImpact` due to a bug in the CRAN version of `devtools`.  Get it with `devtools::install_github("hadley/devtools")` )

```{r}


dwi.ci <- select(dwi,date,count,rev)

pre.ci <- as.Date(c("2012-04-01", "2014-05-31"))
post.ci <- as.Date(c("2014-06-01", "2016-05-09"))

impact <- CausalImpact(dwi.ci, pre.ci, post.ci,  model.args = list( prior.level.sd = 0.1,nseasons = 12, season.duration = 1))

plot(impact)

impact
```

CausalImpact attributes a 10% drop in DWI's to ridesharing.

UNDERSTAND HOW THIS WORKS MORE AND TALK ABOUT IT

Let's look and see if Uber and Lyft leaving had any effect.

```{r}


dwi.ci <- select(dwi,date,count,rev)

pre.ci <- as.Date(c("2014-05-31", "2016-05-09"))
post.ci <- as.Date(c("2016-05-10", "2016-12-31"))

impact <- CausalImpact(dwi.ci, pre.ci, post.ci,model.args = list( prior.level.sd = 0.1))
plot(impact)

impact
```

According to this, voting out Uber and Lyft led to a 5% drop in DWI's in the second half of 2016, which seems counterintuitive.

It's possible that TABC revenues are a bad control variable here, let's see if narcotics arrests are better.

```{r}

ggplot(data=filter(filter(dwi, date > '2012-04-01'),date < '2016-05-09'), mapping = aes(x=date,y=narc, color = year(date))) + uber + geom_point() + geom_smooth(method = 'lm', se = FALSE)

ggplot(data=filter(filter(dwi, date > '2012-04-01'),date < '2016-05-09'), mapping = aes(x=narc,y=count)) + geom_point() + geom_smooth(method = 'lm')

summary(lm(data=filter(filter(dwi, date > '2012-07-01'),date < '2016-05-09'), count ~ narc))

```

In this case, we have a positive correlation, and again no obvious discontinuity.

And our impact test with narcotics arrests as a control:

```{r}

dwi.ci.n <- filter(select(dwi,date,count,narc),date > '2012-04-01')

pre.ci <- as.Date(c("2012-04-01", "2014-05-31"))
post.ci <- as.Date(c("2014-06-01", "2016-05-09"))

impact.n <- CausalImpact(dwi.ci.n, pre.ci, post.ci,model.args = list(prior.level.sd = 0.1,nseasons = 12, season.duration = 1))
plot(impact.n)

impact.n

```

Using narcotics arrests as a control, ridesharing can account for a 7% drop in DWI arrests.

##Regression Discontinuity Design

Regression Discontinuity Design is another method of establishing causality without the benefit of experimental design.  The key assumption is that, with regards to a time series, the data on either side of a treatment inflection are otherwise similar - nothing else special has happened.  We'll look at regressions on either side of the treatment and see how they look - first with just LM and then with a specialized library.

```{r}
ggplot(data = filter(dwi,date > '2012-04-01'),mapping = aes(date, count, color = treatment) )+ geom_point() + geom_smooth(method = 'lm')
```

Visually, you might get a sense of the effect of ridesharing over the period marked in green, but let's keep looking.

```{r}
dwi$index <- seq.int(nrow(dwi))

dwi.w <- filter(dwi, date < '2016-05-01', date > '2012-04-1')
dwi.wo <- filter(dwi, date < '2016-12-31', date > '2014-05-30')

dwi.rdd.w <- rddtools::rdd_data(y = dwi.w$count,x = dwi.w$index,cutpoint = 77)
dwi.rdd.wo <- rddtools::rdd_data(y = dwi.wo$count,x = dwi.wo$index,cutpoint = 101)

reg_para.w <- rdd_reg_lm(rdd_object = dwi.rdd.w, order = 1)
reg_para.wo <- rdd_reg_lm(rdd_object = dwi.rdd.wo, order = 1)

print(reg_para.w)
plot(reg_para.w)

print(reg_para.wo)
plot(reg_para.wo)
```

Checking the p-values for the regressions on either side of both discontinuities reveals that both aren't significantly differerent from random chance.

###References

Deterring Drunk Driving Fatalities: An Economics of Crime Perspective
BRUCE L. BENSON AND DAVID W. RASMUSSEN

Inferring causal impact using Bayesian structural time-series models
Kay H. Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, Steven L. Scott

Regression Discontinuity Designs in Economics
David S. Lee and Thomas Lemieux

#DRAFT DRAFT DRAFT
