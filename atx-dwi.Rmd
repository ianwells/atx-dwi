---
title: "DWI Arrests, TABC Mixed Beverage Taxes, and Ridesharing, in Austin, TX"
output: html_document
date: 2016-01-12
---

#DRAFT DRAFT DRAFT

(Playing along at home?  Here's what you'll need:)

```{r}
(.packages())
select <- dplyr::select
```

```{r include = FALSE}    
library(ggplot2)
library(dplyr)
library(knitr)
library(lubridate)
library(reshape)
library(CausalImpact)
library(rddtools)

select <- dplyr::select 
```

##Background

So, as you may or may not know, Uber and Lyft, America's two premiere ridesharing services, stopped offering rides in Austin, TX as of May 9, 2016 after a contentious public vote which you can read about here: http://www.bizjournals.com/austin/news/2016/05/07/uber-lyft-defeated-in-prop-1-referendum.html

A common argument is that the presence of ride-sharing services such as Uber or Lyft decreases the incidence of drunken driving in a city in which they operate; it sounds reasonable enough to say that if there are more options for getting home after a drink or two or ten, at least one of them might be more attractive than driving home in a potentially compromised state.

There have been a few articles and a couple papers studying the effect of the availability of ridesharing services on drunk driving (see references below), but in my opinion they fall short in one or more ways:

1) They fail to count all available records of drunk driving (like if, say, a study only counted drunk driving fatalities)
2) They fail to account for trends in drunk driving frequency (like if, say, more people than average drove drunk on certain holidays) 
3) They fail to control for exogenous effects on drunk driving frequency (like if, say, the population of a core drinking demographic increased)
4) They fail to properly treat the regression disontinuity between data before and after May 9, 2016 (more on this later)

I've managed to get ahold of three datasets that I think will be helpful:
  
1) APD Incident Reports for 2008-2011, Jan-July 2015, Jan-July 2016 (including a number of intoxication-related crimes)
2) TABC Mixed Beverage Tax Receipts for Austin, 2008-2016.
3) APD Chief's Monthly Reports, 2010-2016, with DWI numbers per month.

The APD (Austin Police Department) Incident Reports are publicy available for years 2008-2011, and for 'YTD' which supposedly includes the last 18 months of reported crime in Austin but actually only includes 8 months of 2016 and a small number of reports from 2015.  I was able to obtain an archived version of the data from late 2015 (https://durs.carto.com/tables/apd_incident_extract_2015csv/public), which has more complete data for Jan-Jul 2015.  It's fair to note that these are not *convictions*, so potentially the number of actual drunk driving incidents is lower - although I'm sure there are people who drive drunk and manage to get home safe without encountering the police.  We may have to do without perfect data on how many people are really driving drunk.

SOMETHING HERE ABOUT DWI ARRESTS AS A PROXY FOR DRUNK DRIVING

The TABC (Texas Alcoholic Beverage Commission) Mixed Beverage Tax is a fixed tax charged on alcoholic beverages sold by a holder of a Mixed Beverage License in the state of Texas.  I've had some fun with this data in the past (see references)!  One of my assumptions is that Mixed Beverage Tax revenues are a good proxy for Units of Alcohol Consumed by Persons at a Place They Had to Travel To, or at least is very correlated with this, assuming an even distribution over time of the price of alcohol (I will control for inflation), the popularity of establishments that hold a Mixed Beverage License vs just a beer-and-wine license (I don't have tax data for these), and the popularity of going out vs drinking at someone's home.  Like with the APD data, what we've got is what we've got.

The APD Chief's Monthly Report is a high-level summary of crime in Austin for the month, including counts for high-profile crimes including DWI.  It has no time-of-day or location information, and no info other than just that a DWI occured, but is another source of data to compare to the incident reports, which may be spotty.

Here are some important dates:

March 2013 : Uber is around for SXSW.
March 2014 : Uber is around for SXSW.

June 4, 2014 : Uber launches in Austin.
https://twitter.com/Uber_ATX/status/474199740217716736

June 3, 2014: Lyft launches in Austin.
https://twitter.com/AustinLyft/status/473857057720766464

May 9, 2016: Uber and Lyft stop giving rides in Austin.

You can find the same data I used here:
1) https://data.austintexas.gov/browse?q=apd&sortBy=relevance
2) https://www.comptroller.texas.gov/transparency/open-data/search-datasets/
3) https://austintexas.gov/page/chiefs-monthly-reports

Or you can contact me directly.

Alright, Let's get to it.

#The Data

##DWI Arrests

```{r include = FALSE}
setwd('/Users/ianwells/sandbox/atx-dwi/')
dwi.raw <- read.csv('data/dwi-raw-nohitandrun-nodre.csv')

sapply(dwi.raw,class)
head(dwi.raw)
dwi.raw$date <- parse_date_time(dwi.raw$date,'%Y-%m-%d %H:%M:%S',tz ='America/Chicago')

har.raw <- read.csv('data/hitandrun-raw.csv')

sapply(har.raw,class)
head(har.raw)
har.raw$date <- parse_date_time(har.raw$date,'%Y-%m-%d %H:%M:%S',tz ='America/Chicago')


tabc.raw <- read.csv('data/tabc-raw.csv')
sapply(tabc.raw,class)
tabc.raw$date <- parse_date_time(tabc.raw$date,'%Y-%m-%d %H:%M:%S', tz = 'America/Chicago')

apd.raw <- read.csv('data/chief-report-raw.csv')

```

Here's a quick glimpse of what the raw APD data looks like:

```{r}
sample_n(dwi.raw,10)
```

Cool.  We have a bunch of crimes, pre-filtered to intoxication-related ones and the time they approximately occured.

You can check this against the full list of crimes availible to commit in Austin, which is available on that same Austin Data Portal site, I'm not including it here because it's quite long, please trust that I've got all of them.

```{r}
levels(dwi.raw$crime)
```

If you're familar with this data set you'll notice I've removed a few crimes:

1) `BOATING WHILE INTOXICATED` - This rare crime doesn't seem like it could be avoided by hailing an Uber.
2) `DWI - DRUG RECOGNITION EXPERT` - You don't have to have any alcohol in your system to be charged with a DWI: in this case, an officer certified in recognizing the effects of some other substance has decided you shouldn't be driving.  While I imagine you could avoid trouble by hailing a ride in this case, I don't think I could relate it to bar sales, so this stat is going to be relegated to some other study.
3) `CRASH/LEAVING THE SCENE` - This is a tricky one.  Certainly a non-zero percentage of hit-and-run offenses are commited by drunk drivers, but this charge doesn't offer any more details, and worse, there are more of this kind of crime than all the others put together.  How should we count them, if at all?  For now, let's leave them all out, as they're not counted in offical DWI tallies.  We might be able to project how many of these crimes involve alcohol at a later date, based on when and where they occured, but we'll leave that for another analysis.

Let's take a look at the relative frequency of these offenses.

```{r warning= FALSE}
qplot(hour(date), data=dwi.raw, geom="histogram", bins = 24)
```

Perhaps not surprisingly, nearly all DWI offenses occur between 8 PM and 5 AM.

```{r warning= FALSE}
qplot(wday(date,label = TRUE), data=dwi.raw, bins = 7)
```

Again, looks like Fridays and Saturdays are popular days to get caught drunk driving.

Let's get down to business.  We'll total everything up by month and see how it looks.  We're going to look at the whole series, and then each year stacked year-over-year, with a local regression applied to see if we can detect a yearly trend, as you might expect some months to be busier than others.

Note: we have incomplete data for July 2015 and August 2016, as well as an old offense in 2014, so we'll remove those months now.

```{r  warning= FALSE}
dwi.raw$year <- year(dwi.raw$date)
dwi.raw$month <- month(dwi.raw$date)
  
dwi.monthly <- dwi.raw %>% group_by(year,month)
dwi.monthly <- summarize(dwi.monthly,count=n())
dwi.monthly$date <- as.Date(paste0(dwi.monthly$year,'-',dwi.monthly$month,'-01'))

dwi.monthly <- dwi.monthly[-56,] 
dwi.monthly <- dwi.monthly[-63,] 
dwi.monthly <- dwi.monthly[-49,] 

label.months <- c('J','F','M','A','M','J','J','A','S','O','N','D')

ggplot(data=dwi.monthly, aes(x = date, y = count, group = year(date), color = factor(year(date)))) + geom_point()

#ggplot(data=dwi.monthly, aes(x = (month(date)), y = count, group = year(date), color = factor(year(date)))) + geom_smooth(se = FALSE) + scale_x_continuous( breaks = 1:12, labels = label.months)

qplot(month(date), data=filter(dwi.monthly,date < '2015-01-01'), geom="histogram", weight = count, bins = 12)


```

So we can get a sense that, for the years 2008-2011, monthly DWI's seem to be decreasing (good job APD), and as for a monthly trend it looks pretty uneven, but not periodic.

Now, I'm curious to see how the Chief's Monthly Report compares.

```{r}

apd.raw$date <- as.Date(apd.raw$date)
apd.raw$year <- year(apd.raw$date)
apd.raw$month <- month(apd.raw$date)

apd.monthly <- apd.raw
  
uber <- geom_rect(mapping = aes(xmin=as.Date('2014-06-01'), xmax = as.Date('2016-05-01')), ymin=-Inf, ymax=+Inf, color="grey20", alpha=0.3, inherit.aes = FALSE)

ggplot(data=apd.monthly, aes(x = date, y = count, color = factor(year))) + geom_point() 

#ggplot(data=apd.monthly, aes(x = month, y = count, group = year, color = factor(year))) + geom_smooth(se = FALSE) + scale_x_continuous(breaks = 1:12, labels = label.months)

qplot(month(date), data=apd.monthly, geom="histogram", weight = count, bins = 12)
```


This looks flatter year-over-year, which isn't what I was expecting.  Let's plot both at the same time.

```{r}

both.monthly <- merge(apd.monthly,dwi.monthly, by = 'date', all = TRUE);
both.monthly$apd <- both.monthly$count.x
both.monthly$dwi <- both.monthly$count.y

both.monthly <- both.monthly %>% select(date,apd,dwi)
both.melted <-melt(both.monthly, id.vars="date")

ggplot(data=both.melted, mapping = aes(x=date,y=value, color = variable)) + geom_point()

```

Okay, so it looks like the chief's report is a little different than the incident report database.  That's annoying, but based on the disclaimers on both websites, and that the chief's report is updated more often, I'm going to call it and say the chief's data is what I'm using moving forward.  I'd like those extra two years of data that the database has though - and since the data is so historic, and the difference is so consistent, what I'm going to do is nudge the data up slightly so that it lines up, and use that.

```{r}

both.monthly$diff <- both.monthly$apd - both.monthly$dwi
mean(filter(both.monthly,date < '2012-01-01', date > '2010-01-01')$diff)

both.monthly.nudge <- both.monthly
both.monthly.nudge$dwi = both.monthly.nudge$dwi + 15

both.melted.nudge <-melt(both.monthly.nudge, id.vars="date")
ggplot(data=both.melted.nudge, mapping = aes(x=date,y=value, color = variable)) + geom_point()

```

Alright, that's a little cleaner.  If anyone wants to be really rigorous, this might be a place to look for mistakes.  I've saved out our final data set here to save time.

```{r}
dwi <- read.csv('data/dwi-final.csv')
dwi$date <- as.Date(dwi$date)
```

## TABC Mixed Beverage Tax

Let's have a look at what's going on with the TABC data.  Now, I've already done a bit of work with this data set (tabcmap.s3-website-us-east-1.amazonaws.com), so I've just pulled values from my database of monthly revenues calculated from monthly tax receipts.

Here's what a few rows look like:

```{r}
sample_n(tabc.raw,10)
```

Let's do the same all-time and then year-over-year plots to get a sense of what's in there.

There's a lone data point from 1999, let's clean that up too, and let's scale revenues into the millions of dollars.

```{r warning=FALSE}
tabc.monthly <- tabc.raw[-40,] 

tabc.monthly$date <- as.Date(tabc.monthly$date)

tabc.monthly$year <- year(tabc.monthly$date)
tabc.monthly$month <- month(tabc.monthly$date)
tabc.monthly$rev <- round(tabc.monthly$rev/1000000,2)

ggplot(data=tabc.monthly, aes(x = date, y = rev, group = year, color = factor(year))) + geom_point()

qplot(month(date), data=tabc.monthly, geom="histogram", weight = rev, bins = 12)
```

Look at that growth!  Notice that spike in March?  That's SXSW, a major annual music, technology, and film conference.  See that spike in October?  That's Austin City Limits, a major music festival.  This data is already good to go, so let's rename it to keep things clean and keep going.

```{r}
tabc <- select(tabc.monthly,date,rev)
```

##DWI per $MM

Let's see about a new variable - DWI per Millions of Dollars Bar Revenue.  Feature engineering, right?

```{r warning = FALSE}
dwi <- merge(dwi,tabc, by = 'date')
dwi$count_per_mm <- dwi$count/dwi$rev

ggplot(data=dwi, aes(x = date, y = count_per_mm, group = year(date), color = factor(year(date)))) + geom_point()

ggplot(data=dwi, aes(x = rev, y = count, color = year(date))) + geom_point() + geom_smooth(method = "lm")

summary(lm(data = dwi, count ~ rev))

```

So while DWI/$MM is clearly decreasing, it would appear that there's no correlation between DWI counts and bar revenue.  Cool!  We'd maybe expect to see annual DWI spikes in March if there was, but we don't.

#The Analysis

Alright, so now that we've got some data, what can we do with it?  How can we establish one way or the other that the advent of ridesharing affected the number of DWI arrests in Austin?  There are a few things I'm going to look at.

##Holt-Winters Forecasting

Holt-Winters Forecasting is a method of predicting time-series data that has strong periodic components.  We saw above that our DWI data has some yearly trends in it - the data shows it, and you can even Google around for articles about DWI in Austin and see for yourself that January is a special month for the police (look for 'no-refusal weekend').

What I'm going to do is feed DWI data from January 2008 - May 2014 (the last month before Uber and Lyft were widely available in Austin) into a Holt-Winters model and forecast DWI counts for the next few months.  I'm then going to compare those to the actual data, with the reasoning that since the model takes into account yearly cycles and broad trends, it will come up with a prediction for DWI counts as if Uber and Lyft never arrived.  We can call the difference the number of DWI's ridesharing prevented, with some degree of confidence.

```{r}


dwi.pre <- filter(dwi,date < '2014-05-31')
dwi.post <- filter(dwi,date > '2014-05-31')

dwi.pre.count.ts <- ts(dwi.pre$count, start = c(2008, 1), frequency = 12)
dwi.post.count.ts <- ts(dwi.post$count, start = c(2014, 6), frequency = 12)
#dwi.rev.ts <- ts(dwi$rev, start = c(2008, 1), frequency = 12)

#plot(dwi.pre.count.ts)

dwi.pre.count.hw <- HoltWinters(dwi.pre.count.ts)
#plot(dwi.pre.count.hw)

#dwi.rev.hw <- HoltWinters(dwi.rev.ts)
#plot(dwi.rev.hw)

forecast <- predict(dwi.pre.count.hw, n.ahead = 8, prediction.interval = T, level = 0.7)
plot(dwi.pre.count.hw, forecast)
lines(dwi.post.count.ts,col='green')

```

ADD RMSE/EVAL FOR MODEL

The forecast is pretty rough - the data may not have been as periodic as it looked.  For the first 8 months Uber and Lyft were available in Austin, the real cumulative DWI count was 158 less than the predicted count - about 4%.  For the first 3 months, where our confidence interval is much tighter, we're only 24 DWI's less than predicted - a decrease of 1.5%, nothing to write home about at all.  

Let's try the back end - fit a model from data starting from Uber's first month, and then stopping on Uber's last month.  If the prediction is lower than the real data, we might be able to say that ridesharing was having an effect, but there was some other factor contaminating the data from 2008-2014 (say an organized campaign against drunk driving or something).

We're going to cheat a tiny bit because we need another sample of data (Holt-Winters requires 2 full years of data here) - so I'm going to say Uber actually opened for business a month earlier than it did.  Hopefully the first month was relatively slow as people started adopting to ridesharing, and it won't make much of a difference to stretch the effect out over one more month in addition the 23 months Uber and Lyft were in Austin.

```{r}

dwi.during <- dwi %>% filter(date >= '2014-5-01') %>% filter(date < '2016-05-01')
dwi.exit <- filter(dwi,date >= '2016-05-01')

dwi.during.count.ts <- ts(dwi.during$count, start = c(2014, 5), frequency = 12)
dwi.exit.count.ts <- ts(dwi.exit$count, start = c(2016, 5), frequency = 12)

dwi.during.count.hw <- HoltWinters(dwi.during.count.ts)

forecast <- predict(dwi.during.count.hw, n.ahead = 6, prediction.interval = T, level = 0.7)
plot(dwi.during.count.hw, forecast)
lines(dwi.exit.count.ts,col='green')

#WHAT'S GOING ON WITH THE SCALE HERE?
```

This looks better - the model fits better, the confidence interval is tighter, and the treatment effect is 80 DWI's over the first 3 months, a decrease of about 6%.  Maybe more convincing than the initial effect, but let's see if we can do better.


##Bayesian Structural Time-series
###With Google CausalImpact

Google has a great library for testing the causal effect of a given treatment on a time series.  Designed for testing the effectiveness of advertising, I think this library will help us out - it's designed for inferrence on non-experimental data, which is what we've got.

Typically, establishing causation requires good experimental design.  We don't have the resources to do a city-scale ridesharing/drunk-driving experiment, so we'll have to use some fancy Google math and our existing data.

For this, we require a response and predictor - let's use DWI count as a response, and TABC revenue as a control, as it appears TABC sales growth is linear through the period Uber was available in Austin, and we've already shown that DWI counts and TABC revenues aren't correlated (PROVE THIS MORE STRONGLY - REMOVE SXSW and ACL?  GROWTH IS SEPERATE?) .

(If you're running this at home note that you'll need the development version of `devtools` to install `CausalImpact` due to a bug in the CRAN version of `devtools`.  Get it with `devtools::install_github("hadley/devtools")` )

```{r}


dwi.ci <- select(dwi,date,count,rev)

pre.ci <- as.Date(c("2008-01-01", "2014-05-31"))
post.ci <- as.Date(c("2014-06-01", "2016-05-09"))

impact <- CausalImpact(dwi.ci, pre.ci, post.ci)
plot(impact)

impact
```

God bless Google for making this so easy.  CausalImpact attributes a 9% drop in DWI's to ridesharing.

UNDERSTAND HOW THIS WORKS MORE AND TALK ABOUT IT

Let's look and see if Uber and Lyft leaving had any effect.

```{r}


dwi.ci <- select(dwi,date,count,rev)

pre.ci <- as.Date(c("2014-05-31", "2016-05-09"))
post.ci <- as.Date(c("2016-05-10", "2016-12-31"))

impact <- CausalImpact(dwi.ci, pre.ci, post.ci)
plot(impact)

impact
```

According to this, voting out Uber and Lyft led to a 5% drop in DWI's in the second half of 2016, which seems counterintuitive.

##Regression Discontinuity Design

###References
Deterring Drunk Driving Fatalities: An Economics of Crime Perspective
BRUCE L. BENSON AND DAVID W. RASMUSSEN

Inferring causal impact using Bayesian structural time-series models
Kay H. Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, Steven L. Scott

Regression Discontinuity Designs in Economics
David S. Lee and Thomas Lemieux

#DRAFT DRAFT DRAFT
